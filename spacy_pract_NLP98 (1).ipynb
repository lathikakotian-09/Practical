{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 1 : TOKENIZATION**"
      ],
      "metadata": {
        "id": "82KY4eHtikHs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf9toWgHhfFm",
        "outputId": "5de3a071-e155-41b5-b8aa-cbac840cc67a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Lathika', 'Kotian', '.', 'Welcome', 'to', 'NCRDs', 'Sterling', 'Institute', 'of', 'Management', 'Studies', '!', '!', '!']\n"
          ]
        }
      ],
      "source": [
        "# 1b.) TOKENIZATION using SpaCy\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "data1 = \"Hello Lathika Kotian. Welcome to NCRDs Sterling Institute of Management Studies !!!\"\n",
        "doc = nlp(data1)\n",
        "\n",
        "# Tokenize using spaCy\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 2 : STOP WORD REMOVAL**"
      ],
      "metadata": {
        "id": "6msdMKyDic0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PRACTICAL-2 SpaCy(Stopword Removal) by Lathika Kotian - 98\\n\")\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\") # Load the spaCy English model\n",
        "\n",
        "data = \"This is a simple example to demonstrate stop word removal in NLP. Success comes to those who work hard and stay focused on their goals\"\n",
        "doc = nlp(data)# Process the text\n",
        "\n",
        "# List of spaCy's stop words\n",
        "stop_words = nlp.Defaults.stop_words\n",
        "print(f\"Stopwords:\\n{stop_words}\\n\")\n",
        "\n",
        "# Filter out stop words\n",
        "filtered_data = [token.text for token in doc if not token.is_stop]\n",
        "\n",
        "print(\"\\nFiltered data after removing stopWords:\\n\",filtered_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsszs56iiYw4",
        "outputId": "978fbe73-5443-4e17-efc3-283abcc151c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRACTICAL-2 SpaCy(Stopword Removal) by Lathika Kotian - 98\n",
            "\n",
            "Stopwords:\n",
            "{'mine', 'from', 'so', 'yourselves', 'there', 'never', 'well', 'me', 'therein', 'how', 'seem', 'would', 'until', 'most', 'eight', 'five', 'get', 'us', 'someone', 'make', 'often', 'through', 'almost', 'those', \"'ve\", 'several', 'two', 'while', 'nowhere', 'too', 'amount', 'wherein', 'own', 'no', 'hereafter', 'behind', 'enough', 'afterwards', '‘re', 'already', 'everyone', '’s', 'around', 'during', 'hers', 'but', 'every', 'much', 'thus', 'whose', \"'s\", 'of', 'something', 'via', 'themselves', 'both', 'seemed', 'third', 'except', 'per', 'he', 'above', 'yourself', 'hereby', 'ourselves', 'your', 'them', 'nor', 'upon', 'one', 'whenever', 'otherwise', 'fifty', 'herself', 'thereafter', 'whom', 'can', 'few', '’ll', 'show', 'used', 'name', 'indeed', 'serious', 'due', 'become', 'quite', 'whereafter', 'does', '‘m', 'which', 'may', 'before', 'wherever', 'amongst', 'anywhere', 'itself', \"'m\", 'at', 'rather', 'call', 'sometime', 'therefore', 'keep', 'anyway', 'my', 'also', 'side', 'such', 'among', 'hereupon', 'should', 'when', '‘d', 'what', 'twenty', 'must', 'yet', '’re', 'either', '’d', 'mostly', 'first', 'became', 'here', 'hundred', 'its', 'unless', 'put', 'within', 'less', 'because', 'else', 'somehow', 'if', 'again', 'together', 'say', 'her', 'against', 'herein', 'nine', 're', 'has', 'made', '‘ve', 'are', \"'ll\", 'ca', 'last', 'have', 'whatever', 'others', 'ten', 'whereas', 'seems', 'even', 'after', 'myself', 'on', 'same', 'besides', 'by', 'neither', 'off', 'along', 'ours', 'meanwhile', 'thereby', 'an', 'more', 'just', 'why', 'eleven', 'throughout', 'give', 'out', 'cannot', \"'d\", 'very', 'always', 'these', 'might', 'thereupon', 'six', 'with', 'down', 'although', 'twelve', 'we', 'between', 'thru', 'over', 'take', 'go', 'becomes', 'full', 'under', 'their', '‘s', 'though', \"'re\", 'some', 'not', 'and', 'to', 'yours', 'regarding', 'that', 'another', 'doing', 'did', 'or', 'across', 'everywhere', '’ve', 'see', 'once', 'other', 'back', 'they', 'below', 'been', 'a', 'least', 'himself', 'whole', 'ever', 'about', 'many', 'using', 'beforehand', 'beside', 'hence', 'alone', 'front', 'into', 'you', 'him', 'fifteen', \"n't\", 'perhaps', 'will', 'his', 'former', 'do', 'still', 'then', 'toward', 'please', 'had', 'n‘t', 'sixty', 'i', 'am', 'thence', 'up', 'done', 'any', 'empty', 'becoming', 'top', 'whoever', 'moreover', 'in', '‘ll', 'now', 'latter', 'were', 'where', 'could', 'noone', 'bottom', 'seeming', 'none', 'whence', 'who', 'since', 'each', 'it', 'only', 'nobody', 'three', 'towards', 'for', 'somewhere', 'nevertheless', 'namely', 'as', 'whither', '’m', 'really', 'whereupon', 'sometimes', 'the', 'whether', 'four', 'forty', 'latterly', 'further', 'n’t', 'everything', 'was', 'nothing', 'part', 'all', 'move', 'anyhow', 'being', 'our', 'than', 'she', 'be', 'various', 'anyone', 'this', 'is', 'without', 'elsewhere', 'beyond', 'formerly', 'however', 'next', 'whereby', 'anything', 'onto'}\n",
            "\n",
            "\n",
            "Filtered data after removing stopWords:\n",
            " ['simple', 'example', 'demonstrate', 'stop', 'word', 'removal', 'NLP', '.', 'Success', 'comes', 'work', 'hard', 'stay', 'focused', 'goals']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 3 : PORTER STEMMER not in spacy**"
      ],
      "metadata": {
        "id": "rn54-_VSisNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 4 : LEMMATIZATION**"
      ],
      "metadata": {
        "id": "u-DFe0ZEkAya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PRACTICAL-4 spaCy (Lemmatizer) by Lathika Kotian - 98\\n\")\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "words = [\"socks\", \"corpora\", \"better\"]\n",
        "# Process each word and print its lemmatized form\n",
        "for word in words:\n",
        "    doc = nlp(word)  # Process word through spaCy\n",
        "    print(f\"{word} :\", doc[0].lemma_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed7X8bNTlAhL",
        "outputId": "c72bb7ca-2101-44be-ba3d-8c97336264ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRACTICAL-4 spaCy (Lemmatizer) by Lathika Kotian - 98\n",
            "\n",
            "socks : sock\n",
            "corpora : corpora\n",
            "better : well\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 5 : Ngram**"
      ],
      "metadata": {
        "id": "AwnHnk9ulS2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PRACTICAL-5 spaCy (N-GRAM Model) by Lathika Kotian - 98\\n\")\n",
        "import spacy\n",
        "from spacy.util import minibatch\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "data = \"The movie was exciting and thrilling\"\n",
        "doc = nlp(data)\n",
        "\n",
        "tokens = [token.text for token in doc] # Convert tokens to list of strings\n",
        "\n",
        "n = 3\n",
        "ngrams = zip(*[tokens[i:] for i in range(n)])\n",
        "\n",
        "print(\"TriGram Model : \")\n",
        "for gram in ngrams:\n",
        "    print(\"\\t\",gram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9_VPHWTlWZD",
        "outputId": "99da58f2-04be-47e0-b0c6-f7d9d95f9ce7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRACTICAL-5 spaCy (N-GRAM Model) by Lathika Kotian - 98\n",
            "\n",
            "TriGram Model : \n",
            "\t ('The', 'movie', 'was')\n",
            "\t ('movie', 'was', 'exciting')\n",
            "\t ('was', 'exciting', 'and')\n",
            "\t ('exciting', 'and', 'thrilling')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 6 : POS Tagging**"
      ],
      "metadata": {
        "id": "Tv6coeOblzj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PRACTICAL-6 SpaCy (POS Tagging) by Lathika Kotian - 98\\n\")\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "print(\"Tokens/Words :-\", [token.text for token in doc])\n",
        "\n",
        "print(\"\\nWord with POS :-\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text} : {token.pos_}\")\n",
        "\n",
        "from spacy import displacy\n",
        "\n",
        "displacy.serve(doc, style='dep')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 933
        },
        "id": "YTIsAyTel21t",
        "outputId": "bbcf86bf-ee77-4520-eb6d-528fa2284ab7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRACTICAL-6 SpaCy (POS Tagging) by Lathika Kotian - 98\n",
            "\n",
            "Tokens/Words :- ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
            "\n",
            "Word with POS :-\n",
            "The : DET\n",
            "quick : ADJ\n",
            "brown : ADJ\n",
            "fox : NOUN\n",
            "jumps : VERB\n",
            "over : ADP\n",
            "the : DET\n",
            "lazy : ADJ\n",
            "dog : NOUN\n",
            ". : PUNCT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/displacy/__init__.py:106: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
            "  warnings.warn(Warnings.W011)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
              "<html lang=\"en\">\n",
              "    <head>\n",
              "        <title>displaCy</title>\n",
              "    </head>\n",
              "\n",
              "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
              "<figure style=\"margin-bottom: 6rem\">\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"fd6426cdaa1a4a2a9016c83322135f50-0\" class=\"displacy\" width=\"1625\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">quick</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">brown</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">fox</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">jumps</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">over</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">lazy</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">dog.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-fd6426cdaa1a4a2a9016c83322135f50-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 575.0,2.0 575.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-fd6426cdaa1a4a2a9016c83322135f50-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-fd6426cdaa1a4a2a9016c83322135f50-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-fd6426cdaa1a4a2a9016c83322135f50-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-fd6426cdaa1a4a2a9016c83322135f50-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-fd6426cdaa1a4a2a9016c83322135f50-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-fd6426cdaa1a4a2a9016c83322135f50-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-fd6426cdaa1a4a2a9016c83322135f50-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-fd6426cdaa1a4a2a9016c83322135f50-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-fd6426cdaa1a4a2a9016c83322135f50-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-fd6426cdaa1a4a2a9016c83322135f50-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,89.5 1445.0,89.5 1445.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-fd6426cdaa1a4a2a9016c83322135f50-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-fd6426cdaa1a4a2a9016c83322135f50-0-6\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-fd6426cdaa1a4a2a9016c83322135f50-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1295,266.5 L1287,254.5 1303,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-fd6426cdaa1a4a2a9016c83322135f50-0-7\" stroke-width=\"2px\" d=\"M945,264.5 C945,2.0 1450.0,2.0 1450.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-fd6426cdaa1a4a2a9016c83322135f50-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1450.0,266.5 L1458.0,254.5 1442.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>\n",
              "</figure>\n",
              "</body>\n",
              "</html></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using the 'dep' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n",
            "Shutting down server on port 5000.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 7 :NER**"
      ],
      "metadata": {
        "id": "t7HC6qQKmBo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Name_entity_recognition\n",
        "print(\"PRACTICAL-7 SpaCy (NER) by Lathika Kotian - 98\\n\")\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Lathika Kotian was born in Mumbai, India on November 09, 2003.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Named Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"\\t{ent.text} -> {ent.label_}\")\n",
        "print()\n",
        "# Visualize named entities in Colab\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "hp8tK8qLmErA",
        "outputId": "5179321c-988b-49e1-efc5-62925446540d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRACTICAL-7 SpaCy (NER) by Lathika Kotian - 98\n",
            "\n",
            "Named Entities:\n",
            "\tLathika Kotian -> PERSON\n",
            "\tMumbai -> GPE\n",
            "\tIndia -> GPE\n",
            "\tNovember 09, 2003 -> DATE\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Lathika Kotian\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " was born in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Mumbai\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " on \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    November 09, 2003\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 8A : BOW**"
      ],
      "metadata": {
        "id": "zBMxaVBjmQBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PRACTICAL-8A SpaCy(BOW) by Lathika Kotian - 98\\n\")\n",
        "import spacy\n",
        "import numpy as np\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "texts = [\n",
        "    \"I like to play badminton\",\n",
        "    \"Badminton is a great sport\",\n",
        "    # \"I enjoy playing badminton on weekends.\"\n",
        "]\n",
        "\n",
        "processed_texts = [nlp(text.lower()) for text in texts]\n",
        "\n",
        "# Create vocabulary: unique words (without stopwords or punctuation)\n",
        "vocabulary = sorted(set(token.text for doc in processed_texts for token in doc if token.is_alpha and not token.is_stop))\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "\n",
        "# Create Bag of Words (BoW) representation\n",
        "def get_bow_representation(doc, vocabulary):\n",
        "    return [doc.text.count(word) for word in vocabulary]\n",
        "\n",
        "# Create BoW vectors for each sentence\n",
        "bow_vectors = [get_bow_representation(doc, vocabulary) for doc in processed_texts]\n",
        "\n",
        "# Print BoW vectors\n",
        "print(\"\\nBoW vectors:\")\n",
        "for i, bow_vector in enumerate(bow_vectors):\n",
        "    print(f\"\\tSentence {i+1}: {bow_vector}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sx5WR8y2mTKF",
        "outputId": "e23970be-1862-40d9-f372-e85f317b8a94"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRACTICAL-8A SpaCy(BOW) by Lathika Kotian - 98\n",
            "\n",
            "Vocabulary: ['badminton', 'great', 'like', 'play', 'sport']\n",
            "\n",
            "BoW vectors:\n",
            "\tSentence 1: [1, 0, 1, 1, 0]\n",
            "\tSentence 2: [1, 1, 0, 0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 8B : TF IDF**"
      ],
      "metadata": {
        "id": "qN8t6AqCmdWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PRACTICAL-8B spaCy (TF-IDF) by Lathika Kotian - 98\\n\")\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import log\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "texts = [\n",
        "    \"I like to play badminton\",\n",
        "    \"Badminton is a great sport\",\n",
        "    # \"I enjoy playing badminton on weekends.\"\n",
        "]\n",
        "\n",
        "tokenized_texts = [[token.text.lower() for token in nlp(text) if not token.is_punct and not token.is_space] for text in texts]\n",
        "\n",
        "# Create sorted vocabulary\n",
        "vocabulary = sorted(set(word for text in tokenized_texts for word in text))\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "\n",
        "# TF function\n",
        "def get_tf(tokens, vocabulary):\n",
        "    return [tokens.count(word) for word in vocabulary]\n",
        "\n",
        "# IDF function\n",
        "def get_idf(vocabulary, docs):\n",
        "    num_docs = len(docs)\n",
        "    idf_vector = []\n",
        "    for word in vocabulary:\n",
        "        num_docs_with_word = sum(1 for doc in docs if word in doc)\n",
        "        idf_value = log(num_docs / (1 + num_docs_with_word)) + 1  # Smoothed\n",
        "        idf_vector.append(idf_value)\n",
        "    return idf_vector\n",
        "\n",
        "# TF-IDF function\n",
        "def get_tfidf(tokens, vocabulary, idf_vector):\n",
        "    tf_vector = get_tf(tokens, vocabulary)\n",
        "    return [tf * idf for tf, idf in zip(tf_vector, idf_vector)]\n",
        "\n",
        "# calcu IDF\n",
        "idf_vector = get_idf(vocabulary, tokenized_texts)\n",
        "\n",
        "# Show IDF\n",
        "idf_df = pd.DataFrame({\n",
        "    'Word': vocabulary,\n",
        "    'IDF': np.round(idf_vector, 4)\n",
        "})\n",
        "print(\"\\nIDF Values:\")\n",
        "print(idf_df)\n",
        "\n",
        "# calc TF-IDF for each doc\n",
        "tfidf_matrix = []\n",
        "for tokens in tokenized_texts:\n",
        "    tfidf = get_tfidf(tokens, vocabulary, idf_vector)\n",
        "    tfidf_matrix.append(tfidf)\n",
        "\n",
        "tfidf_df = pd.DataFrame(np.round(tfidf_matrix, 4), columns=vocabulary, index=[f\"Doc{i+1}\" for i in range(len(texts))])\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(tfidf_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSHySrKkmg7U",
        "outputId": "3d4c668f-5f2c-4c4c-f084-459b06d97579"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRACTICAL-8B spaCy (TF-IDF) by Lathika Kotian - 98\n",
            "\n",
            "Vocabulary: ['a', 'badminton', 'great', 'i', 'is', 'like', 'play', 'sport', 'to']\n",
            "\n",
            "IDF Values:\n",
            "        Word     IDF\n",
            "0          a  1.0000\n",
            "1  badminton  0.5945\n",
            "2      great  1.0000\n",
            "3          i  1.0000\n",
            "4         is  1.0000\n",
            "5       like  1.0000\n",
            "6       play  1.0000\n",
            "7      sport  1.0000\n",
            "8         to  1.0000\n",
            "\n",
            "TF-IDF Matrix:\n",
            "        a  badminton  great    i   is  like  play  sport   to\n",
            "Doc1  0.0     0.5945    0.0  1.0  0.0   1.0   1.0    0.0  1.0\n",
            "Doc2  1.0     0.5945    1.0  0.0  1.0   0.0   0.0    1.0  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 8C : COSINE SIMILARITY**"
      ],
      "metadata": {
        "id": "NnR_l1yemuxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "print(\"PRACTICAL-8c Cosine Similarity using SpaCy by Lathika Kotian - 98\\n\")\n",
        "\n",
        "texts = [\n",
        "    \"I like to play badminton\",\n",
        "    \"Badminton is a great sport\"\n",
        "]\n",
        "\n",
        "docs = [nlp(text) for text in texts]\n",
        "\n",
        "# Create a vocab\n",
        "#vocabulary = set(token.text.lower() for doc in docs for token in doc if not token.is_stop and not token.is_punct)\n",
        "vocabulary = set(token.text.lower() for doc in docs for token in doc if not token.is_punct)\n",
        "print(\"Vocabulary: \", vocabulary)\n",
        "\n",
        "# bow\n",
        "def get_bow_representation(doc, vocabulary):\n",
        "    return [doc.text.lower().split().count(word) for word in vocabulary]\n",
        "\n",
        "bow_vectors = [get_bow_representation(doc, vocabulary) for doc in docs]\n",
        "\n",
        "# BoW vectors\n",
        "print(\"\\nBag of Words (BoW) Vectors:\")\n",
        "for i, bow_vector in enumerate(bow_vectors, start=1):\n",
        "    print(f\"Text {i} BoW vector: {np.array(bow_vector)}\")\n",
        "\n",
        "bow_similarity = cosine_similarity([bow_vectors[0]], [bow_vectors[1]])[0][0]\n",
        "print(f\"\\nCosine Similarity between Text 1 and Text 2 is {bow_similarity*100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB_WfXWem4vq",
        "outputId": "d3afa401-fd7e-4a32-b52d-f06069cdd060"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRACTICAL-8c Cosine Similarity using SpaCy by Lathika Kotian - 98\n",
            "\n",
            "Vocabulary:  {'like', 'great', 'sport', 'to', 'a', 'is', 'play', 'i', 'badminton'}\n",
            "\n",
            "Bag of Words (BoW) Vectors:\n",
            "Text 1 BoW vector: [1 0 0 1 0 0 1 1 1]\n",
            "Text 2 BoW vector: [0 1 1 0 1 1 0 0 1]\n",
            "\n",
            "Cosine Similarity between Text 1 and Text 2 is 20.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 8D (BOW + TF-IDF) Cosine Similarity**"
      ],
      "metadata": {
        "id": "cjZmd6Jkm_Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from math import log\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "print(\"PRACTICAL-8D (BOW + TF-IDF) Cosine Similarity using SpaCy by Lathika Kotian - 98\\n\")\n",
        "\n",
        "texts = [\n",
        "    \"I like to play badminton\",\n",
        "    \"Badminton is a great sport\"\n",
        "]\n",
        "\n",
        "docs = [nlp(text) for text in texts]\n",
        "\n",
        "# Create vocab\n",
        "vocabulary = set(token.text.lower() for doc in docs for token in doc if not token.is_punct)\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "\n",
        "#bow\n",
        "def get_bow_representation(doc, vocabulary):\n",
        "    return [doc.text.lower().split().count(word) for word in vocabulary]\n",
        "\n",
        "bow_vectors = [get_bow_representation(doc, vocabulary) for doc in docs]\n",
        "\n",
        "# function TF\n",
        "def get_tf(doc, vocabulary):\n",
        "    return [doc.text.lower().split().count(word) for word in vocabulary]\n",
        "\n",
        "# function IDF\n",
        "def get_idf(vocabulary, docs):\n",
        "    num_docs = len(docs)\n",
        "    idf_vector = []\n",
        "    for word in vocabulary:\n",
        "        num_docs_with_word = sum(1 for doc in docs if word in doc.text.lower().split())\n",
        "        idf_value = log(num_docs / (1 + num_docs_with_word)) + 1\n",
        "        idf_vector.append(idf_value)\n",
        "    return idf_vector\n",
        "\n",
        "#function TF-IDF\n",
        "def get_tfidf(doc, vocabulary, idf_vector):\n",
        "    tf_vector = get_tf(doc, vocabulary)\n",
        "    tfidf_vector = [tf * idf for tf, idf in zip(tf_vector, idf_vector)]\n",
        "    return tfidf_vector\n",
        "\n",
        "# calc IDF fo entire corpus\n",
        "idf_vector = get_idf(vocabulary, docs)\n",
        "\n",
        "# cala TF-IDF for each document\n",
        "tfidf_vectors = [get_tfidf(doc, vocabulary, idf_vector) for doc in docs]\n",
        "\n",
        "# calc cosine similarity between BoW and TF-IDF vectors for doc1\n",
        "bow_similarity = cosine_similarity([bow_vectors[0]], [tfidf_vectors[0]])[0][0]\n",
        "print(f\"\\nCosine Similarity between doc1 (BoW) and doc1 (TF-IDF): {bow_similarity*100:.2f}%\")\n",
        "\n",
        "# clac cosine similarity between BoW and TF-IDF vectors for doc2\n",
        "bow_similarity = cosine_similarity([bow_vectors[1]], [tfidf_vectors[1]])[0][0]\n",
        "print(f\"Cosine Similarity between doc2 (BoW) and doc2 (TF-IDF): {bow_similarity*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "949-A0sRnBuV",
        "outputId": "16e0216b-90fb-43ee-d463-bddab8f7309d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRACTICAL-8D (BOW + TF-IDF) Cosine Similarity using SpaCy by Lathika Kotian - 98\n",
            "\n",
            "Vocabulary: {'like', 'great', 'sport', 'to', 'a', 'is', 'play', 'i', 'badminton'}\n",
            "\n",
            "Cosine Similarity between doc1 (BoW) and doc1 (TF-IDF): 98.48%\n",
            "Cosine Similarity between doc2 (BoW) and doc2 (TF-IDF): 98.48%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 9 WORDTOVEC**"
      ],
      "metadata": {
        "id": "9DyoqKW3neVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGmBfiJboPZx",
        "outputId": "834fb672-0e7c-4b91-db70-23685e1a08f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "print(\"PRACTICAL-9 WordToVec using spaCy by Lathika Kotian - 98\\n\")\n",
        "\n",
        "# Function to train Word2Vec model using spaCy tokenization\n",
        "def train_word_embeddings(sentences):\n",
        "    # Tokenize sentences using spaCy and convert to lowercase\n",
        "    tokenized_sentences = [\n",
        "        [token.text.lower() for token in nlp(sentence) if not token.is_stop and not token.is_punct]\n",
        "        for sentence in sentences\n",
        "    ]\n",
        "\n",
        "    # Train Word2Vec model\n",
        "    model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to use trained Word2Vec model and find similar words\n",
        "def use_word_embeddings(model, word, top_n=5):\n",
        "    try:\n",
        "        # Get the top N similar words to the input word\n",
        "        similar_words = model.wv.most_similar(word, topn=top_n)\n",
        "        print(f\"Words most similar to '{word}':\")\n",
        "        for w, score in similar_words:\n",
        "            print(f\"{w}: {score:.4f}\")\n",
        "    except KeyError:\n",
        "        print(f\"'{word}' not in vocabulary\")\n",
        "\n",
        "# Example usage\n",
        "sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"A fox is a cunning animal\",\n",
        "    \"The dog barks at night\",\n",
        "    \"Foxes and dogs are different species\"\n",
        "]\n",
        "\n",
        "# Train Word2Vec model using the provided sentences\n",
        "model = train_word_embeddings(sentences)\n",
        "\n",
        "# Use the trained model to find words similar to \"fox\"\n",
        "use_word_embeddings(model, \"fox\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMqsthMVnher",
        "outputId": "8b7bd2c8-1fdf-4d5e-baed-5e2df80d81ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRACTICAL-9 WordToVec using spaCy by Lathika Kotian - 98\n",
            "\n",
            "Words most similar to 'fox':\n",
            "quick: 0.1373\n",
            "different: 0.0680\n",
            "animal: 0.0336\n",
            "foxes: 0.0094\n",
            "jumps: 0.0083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 10:  NAIVE BAYES**"
      ],
      "metadata": {
        "id": "ayw0nqrooqmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PRACTICAL-10   text classifier NaiveBayes using scikit learn by Lathika Kotian - 98\\n\")\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "texts = [\n",
        "    \"I love programming\",\n",
        "    \"Python is great\",\n",
        "    \"I hate bugs\",\n",
        "    \"Coding is fun\",\n",
        "    \"I love solving problems\",\n",
        "    \"I hate error messages\",\n",
        "    \"Programming is awesome\",\n",
        "    \"Debugging is boring\",\n",
        "    \"This is terrible\",\n",
        "    \"Very disappointed with the experience\",\n",
        "    \"Worst product ever\",\n",
        "    \"Such a bad service\",\n",
        "    \"Absolutely amazing experience\",\n",
        "    \"Fantastic service and support\",\n",
        "    \"Really enjoyed the performance\",\n",
        "    \"Not satisfied at all after this\"\n",
        "]\n",
        "\n",
        "labels = [\n",
        "    'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative',\n",
        "    'negative', 'negative', 'negative', 'negative',\n",
        "    'positive', 'positive', 'positive','negative'\n",
        "]\n",
        "\n",
        "# Vectorize the text using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "y = np.array(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate MOdel\n",
        "print(f\"Accuracy: {metrics.accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "# Classify new text FOR testing\n",
        "new_texts = [\n",
        "    \"I really enjoyed the movie\",\n",
        "    \"This product is terrible and disappointing\",\n",
        "    \"Absolutely loved it!\",\n",
        "    \"Not satisfied at all\"\n",
        "]\n",
        "\n",
        "new_texts_vectorized = vectorizer.transform(new_texts)\n",
        "new_predictions = classifier.predict(new_texts_vectorized)\n",
        "\n",
        "print(\"\\nNew Text Classification Results:\")\n",
        "for text, label in zip(new_texts, new_predictions):\n",
        "    print(f\"Text: '{text}' => Predicted Label: '{label}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sf9QO2oovbP",
        "outputId": "47127295-d019-4758-cbb8-d291c40a443a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRACTICAL-10   text classifier NaiveBayes using scikit learn by Lathika Kotian - 98\n",
            "\n",
            "Accuracy: 0.7500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1 0]\n",
            " [1 2]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.50      1.00      0.67         1\n",
            "    positive       1.00      0.67      0.80         3\n",
            "\n",
            "    accuracy                           0.75         4\n",
            "   macro avg       0.75      0.83      0.73         4\n",
            "weighted avg       0.88      0.75      0.77         4\n",
            "\n",
            "\n",
            "New Text Classification Results:\n",
            "Text: 'I really enjoyed the movie' => Predicted Label: 'negative'\n",
            "Text: 'This product is terrible and disappointing' => Predicted Label: 'negative'\n",
            "Text: 'Absolutely loved it!' => Predicted Label: 'positive'\n",
            "Text: 'Not satisfied at all' => Predicted Label: 'negative'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 11:SENTIMENT ANALYSIS  using textblob not spacy**"
      ],
      "metadata": {
        "id": "CFxlbyFtpDbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "\n",
        "texts = [\n",
        "    \"I absolutely love this product! It's amazing!\",\n",
        "    \"This is the worst experience I've ever had.\",\n",
        "    \"The movie was okay, nothing special.\",\n",
        "    \"I'm feeling pretty neutral about the whole situation.\",\n",
        "    \"The customer service was excellent and very helpful!\"\n",
        "]\n",
        "\n",
        "results = []\n",
        "for text in texts:\n",
        "    blob = TextBlob(text)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    subjectivity = blob.sentiment.subjectivity\n",
        "    sentiment = \"Positive\" if polarity > 0 else \"Negative\" if polarity < 0 else \"Neutral\"\n",
        "\n",
        "    results.append({\n",
        "        'Text': text,\n",
        "        'Sentiment': sentiment,\n",
        "        'Polarity': polarity,\n",
        "        'Subjectivity': subjectivity\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "print(\"TextBlob Sentiment Analysis Results\\n\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PARr2oNpKht",
        "outputId": "d2d5fa54-910e-4568-98b8-8df05f8b5a08"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextBlob Sentiment Analysis Results\n",
            "\n",
            "                                                Text Sentiment  Polarity  \\\n",
            "0      I absolutely love this product! It's amazing!  Positive  0.687500   \n",
            "1        This is the worst experience I've ever had.  Negative -1.000000   \n",
            "2               The movie was okay, nothing special.  Positive  0.428571   \n",
            "3  I'm feeling pretty neutral about the whole sit...  Positive  0.225000   \n",
            "4  The customer service was excellent and very he...  Positive  0.625000   \n",
            "\n",
            "   Subjectivity  \n",
            "0      0.750000  \n",
            "1      1.000000  \n",
            "2      0.535714  \n",
            "3      0.700000  \n",
            "4      0.650000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 12 : SUMMARIZATION**"
      ],
      "metadata": {
        "id": "-wUXxTOTpPWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PRACTICAL-12: Text summarization using spacy by Lathika Kotian - 98\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "from heapq import nlargest\n",
        "\n",
        "text = \"\"\"\n",
        "Natural Language Processing is a subfield of artificial intelligence (AI) that focuses on enabling machines to understand, interpret, and generate human language. NLP combines computational linguistics, computer science, and machine learning to process and analyze vast amounts of natural language data. As communication through text and speech is fundamental to human interaction, NLP plays a crucial role in bridging the gap between human and machine communication. One of the fundamental tasks in NLP is tokenization, where text is broken down into smaller units such as words or sentences. This is often the first step in more complex processes like part-of-speech tagging, which assigns grammatical categories (like noun or verb) to each word. Other essential techniques include stemming and lemmatization, which reduce words to their root or dictionary form to help normalize variations. More advanced NLP tasks include named entity recognition (NER), which identifies proper nouns like names of people, organizations, and locations, and sentiment analysis, which determines the emotional tone behind a body of text. Text classification, another key task, allows for organizing documents or messages into categories such as spam vs. non-spam, or positive vs. negative reviews. With the advent of deep learning, NLP has advanced dramatically. Particularly transformer-based architectures like BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), and RoBERTa. These models are capable of understanding context, ambiguity, and even generating human-like text with impressive fluency. NLP is widely used in both consumer and enterprise applications. Popular use cases include chatbots and virtual assistants like Siri, Alexa, and Google Assistant, which rely on NLP to interpret and respond to voice commands.\n",
        "\"\"\"\n",
        "\n",
        "stopwords = list(STOP_WORDS)\n",
        "# length of stopwords in spacy\n",
        "print(\"Length of stopwords : \",len(stopwords))\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(text)\n",
        "# word tokenization\n",
        "tokens = [token.text for token in doc]\n",
        "print(\"WORD TOKENIZATION : \",tokens)\n",
        "print(\"-----------------\")\n",
        "# word frequency\n",
        "word_frequency = {}\n",
        "for word in doc:\n",
        "    if word.text.lower() not in stopwords:\n",
        "        if word.text.lower() not in punctuation:\n",
        "            if word.text not in word_frequency.keys():\n",
        "                word_frequency[word.text] = 1\n",
        "            else:\n",
        "                word_frequency[word.text] += 1\n",
        "print(\"WORD FREQUENCY : \",word_frequency)\n",
        "\n",
        "# normalized frequency\n",
        "max_frequency = max(word_frequency.values())\n",
        "for word in word_frequency.keys():\n",
        "    word_frequency[word] = word_frequency[word]/max_frequency\n",
        "print(\"NORMALIZED FREQUENCY : \",word_frequency)\n",
        "print(\"-----------------\")\n",
        "\n",
        "# sentence tokens\n",
        "sentence_tokens = [sent for sent in doc.sents]\n",
        "len(sentence_tokens)\n",
        "\n",
        "sentence_scores = {}\n",
        "for sent in sentence_tokens:\n",
        "    for word in sent:\n",
        "        if word.text.lower() in word_frequency.keys():\n",
        "            if sent not in sentence_scores.keys():\n",
        "                sentence_scores[sent] = word_frequency[word.text.lower()]\n",
        "            else:\n",
        "                sentence_scores[sent] +=word_frequency[word.text.lower()]\n",
        "print(\"SENTENCE SCORE : \",sentence_scores)\n",
        "print(\"Length of Sentence Score : \",len(sentence_tokens))\n",
        "\n",
        "\n",
        "select_length = int(len(sentence_tokens)*0.3)\n",
        "select_length\n",
        "summary = nlargest(select_length,sentence_scores,key=sentence_scores.get)\n",
        "print(summary)\n",
        "print(\"-----------------\")\n",
        "final_summary = [word.text for word in summary]\n",
        "final_summary = ''.join(final_summary)\n",
        "print(\"TEXT SUMMARIZATION : \\n\",final_summary)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pd-5IxQpTst",
        "outputId": "4dec0e7d-f277-4f8a-fdbb-840147a1af59"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of stopwords :  326\n",
            "WORD TOKENIZATION :  ['\\n', 'Natural', 'Language', 'Processing', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '(', 'AI', ')', 'that', 'focuses', 'on', 'enabling', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'NLP', 'combines', 'computational', 'linguistics', ',', 'computer', 'science', ',', 'and', 'machine', 'learning', 'to', 'process', 'and', 'analyze', 'vast', 'amounts', 'of', 'natural', 'language', 'data', '.', 'As', 'communication', 'through', 'text', 'and', 'speech', 'is', 'fundamental', 'to', 'human', 'interaction', ',', 'NLP', 'plays', 'a', 'crucial', 'role', 'in', 'bridging', 'the', 'gap', 'between', 'human', 'and', 'machine', 'communication', '.', 'One', 'of', 'the', 'fundamental', 'tasks', 'in', 'NLP', 'is', 'tokenization', ',', 'where', 'text', 'is', 'broken', 'down', 'into', 'smaller', 'units', 'such', 'as', 'words', 'or', 'sentences', '.', 'This', 'is', 'often', 'the', 'first', 'step', 'in', 'more', 'complex', 'processes', 'like', 'part', '-', 'of', '-', 'speech', 'tagging', ',', 'which', 'assigns', 'grammatical', 'categories', '(', 'like', 'noun', 'or', 'verb', ')', 'to', 'each', 'word', '.', 'Other', 'essential', 'techniques', 'include', 'stemming', 'and', 'lemmatization', ',', 'which', 'reduce', 'words', 'to', 'their', 'root', 'or', 'dictionary', 'form', 'to', 'help', 'normalize', 'variations', '.', 'More', 'advanced', 'NLP', 'tasks', 'include', 'named', 'entity', 'recognition', '(', 'NER', ')', ',', 'which', 'identifies', 'proper', 'nouns', 'like', 'names', 'of', 'people', ',', 'organizations', ',', 'and', 'locations', ',', 'and', 'sentiment', 'analysis', ',', 'which', 'determines', 'the', 'emotional', 'tone', 'behind', 'a', 'body', 'of', 'text', '.', 'Text', 'classification', ',', 'another', 'key', 'task', ',', 'allows', 'for', 'organizing', 'documents', 'or', 'messages', 'into', 'categories', 'such', 'as', 'spam', 'vs.', 'non', '-', 'spam', ',', 'or', 'positive', 'vs.', 'negative', 'reviews', '.', 'With', 'the', 'advent', 'of', 'deep', 'learning', ',', 'NLP', 'has', 'advanced', 'dramatically', '.', 'Particularly', 'transformer', '-', 'based', 'architectures', 'like', 'BERT', '(', 'Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', ')', ',', 'GPT', '(', 'Generative', 'Pre', '-', 'trained', 'Transformer', ')', ',', 'and', 'RoBERTa', '.', 'These', 'models', 'are', 'capable', 'of', 'understanding', 'context', ',', 'ambiguity', ',', 'and', 'even', 'generating', 'human', '-', 'like', 'text', 'with', 'impressive', 'fluency', '.', 'NLP', 'is', 'widely', 'used', 'in', 'both', 'consumer', 'and', 'enterprise', 'applications', '.', 'Popular', 'use', 'cases', 'include', 'chatbots', 'and', 'virtual', 'assistants', 'like', 'Siri', ',', 'Alexa', ',', 'and', 'Google', 'Assistant', ',', 'which', 'rely', 'on', 'NLP', 'to', 'interpret', 'and', 'respond', 'to', 'voice', 'commands', '.', '\\n']\n",
            "-----------------\n",
            "WORD FREQUENCY :  {'\\n': 2, 'Natural': 1, 'Language': 1, 'Processing': 1, 'subfield': 1, 'artificial': 1, 'intelligence': 1, 'AI': 1, 'focuses': 1, 'enabling': 1, 'machines': 1, 'understand': 1, 'interpret': 2, 'generate': 1, 'human': 4, 'language': 2, 'NLP': 7, 'combines': 1, 'computational': 1, 'linguistics': 1, 'computer': 1, 'science': 1, 'machine': 2, 'learning': 2, 'process': 1, 'analyze': 1, 'vast': 1, 'amounts': 1, 'natural': 1, 'data': 1, 'communication': 2, 'text': 4, 'speech': 2, 'fundamental': 2, 'interaction': 1, 'plays': 1, 'crucial': 1, 'role': 1, 'bridging': 1, 'gap': 1, 'tasks': 2, 'tokenization': 1, 'broken': 1, 'smaller': 1, 'units': 1, 'words': 2, 'sentences': 1, 'step': 1, 'complex': 1, 'processes': 1, 'like': 6, 'tagging': 1, 'assigns': 1, 'grammatical': 1, 'categories': 2, 'noun': 1, 'verb': 1, 'word': 1, 'essential': 1, 'techniques': 1, 'include': 3, 'stemming': 1, 'lemmatization': 1, 'reduce': 1, 'root': 1, 'dictionary': 1, 'form': 1, 'help': 1, 'normalize': 1, 'variations': 1, 'advanced': 2, 'named': 1, 'entity': 1, 'recognition': 1, 'NER': 1, 'identifies': 1, 'proper': 1, 'nouns': 1, 'names': 1, 'people': 1, 'organizations': 1, 'locations': 1, 'sentiment': 1, 'analysis': 1, 'determines': 1, 'emotional': 1, 'tone': 1, 'body': 1, 'Text': 1, 'classification': 1, 'key': 1, 'task': 1, 'allows': 1, 'organizing': 1, 'documents': 1, 'messages': 1, 'spam': 2, 'vs.': 2, 'non': 1, 'positive': 1, 'negative': 1, 'reviews': 1, 'advent': 1, 'deep': 1, 'dramatically': 1, 'Particularly': 1, 'transformer': 1, 'based': 1, 'architectures': 1, 'BERT': 1, 'Bidirectional': 1, 'Encoder': 1, 'Representations': 1, 'Transformers': 1, 'GPT': 1, 'Generative': 1, 'Pre': 1, 'trained': 1, 'Transformer': 1, 'RoBERTa': 1, 'models': 1, 'capable': 1, 'understanding': 1, 'context': 1, 'ambiguity': 1, 'generating': 1, 'impressive': 1, 'fluency': 1, 'widely': 1, 'consumer': 1, 'enterprise': 1, 'applications': 1, 'Popular': 1, 'use': 1, 'cases': 1, 'chatbots': 1, 'virtual': 1, 'assistants': 1, 'Siri': 1, 'Alexa': 1, 'Google': 1, 'Assistant': 1, 'rely': 1, 'respond': 1, 'voice': 1, 'commands': 1}\n",
            "NORMALIZED FREQUENCY :  {'\\n': 0.2857142857142857, 'Natural': 0.14285714285714285, 'Language': 0.14285714285714285, 'Processing': 0.14285714285714285, 'subfield': 0.14285714285714285, 'artificial': 0.14285714285714285, 'intelligence': 0.14285714285714285, 'AI': 0.14285714285714285, 'focuses': 0.14285714285714285, 'enabling': 0.14285714285714285, 'machines': 0.14285714285714285, 'understand': 0.14285714285714285, 'interpret': 0.2857142857142857, 'generate': 0.14285714285714285, 'human': 0.5714285714285714, 'language': 0.2857142857142857, 'NLP': 1.0, 'combines': 0.14285714285714285, 'computational': 0.14285714285714285, 'linguistics': 0.14285714285714285, 'computer': 0.14285714285714285, 'science': 0.14285714285714285, 'machine': 0.2857142857142857, 'learning': 0.2857142857142857, 'process': 0.14285714285714285, 'analyze': 0.14285714285714285, 'vast': 0.14285714285714285, 'amounts': 0.14285714285714285, 'natural': 0.14285714285714285, 'data': 0.14285714285714285, 'communication': 0.2857142857142857, 'text': 0.5714285714285714, 'speech': 0.2857142857142857, 'fundamental': 0.2857142857142857, 'interaction': 0.14285714285714285, 'plays': 0.14285714285714285, 'crucial': 0.14285714285714285, 'role': 0.14285714285714285, 'bridging': 0.14285714285714285, 'gap': 0.14285714285714285, 'tasks': 0.2857142857142857, 'tokenization': 0.14285714285714285, 'broken': 0.14285714285714285, 'smaller': 0.14285714285714285, 'units': 0.14285714285714285, 'words': 0.2857142857142857, 'sentences': 0.14285714285714285, 'step': 0.14285714285714285, 'complex': 0.14285714285714285, 'processes': 0.14285714285714285, 'like': 0.8571428571428571, 'tagging': 0.14285714285714285, 'assigns': 0.14285714285714285, 'grammatical': 0.14285714285714285, 'categories': 0.2857142857142857, 'noun': 0.14285714285714285, 'verb': 0.14285714285714285, 'word': 0.14285714285714285, 'essential': 0.14285714285714285, 'techniques': 0.14285714285714285, 'include': 0.42857142857142855, 'stemming': 0.14285714285714285, 'lemmatization': 0.14285714285714285, 'reduce': 0.14285714285714285, 'root': 0.14285714285714285, 'dictionary': 0.14285714285714285, 'form': 0.14285714285714285, 'help': 0.14285714285714285, 'normalize': 0.14285714285714285, 'variations': 0.14285714285714285, 'advanced': 0.2857142857142857, 'named': 0.14285714285714285, 'entity': 0.14285714285714285, 'recognition': 0.14285714285714285, 'NER': 0.14285714285714285, 'identifies': 0.14285714285714285, 'proper': 0.14285714285714285, 'nouns': 0.14285714285714285, 'names': 0.14285714285714285, 'people': 0.14285714285714285, 'organizations': 0.14285714285714285, 'locations': 0.14285714285714285, 'sentiment': 0.14285714285714285, 'analysis': 0.14285714285714285, 'determines': 0.14285714285714285, 'emotional': 0.14285714285714285, 'tone': 0.14285714285714285, 'body': 0.14285714285714285, 'Text': 0.14285714285714285, 'classification': 0.14285714285714285, 'key': 0.14285714285714285, 'task': 0.14285714285714285, 'allows': 0.14285714285714285, 'organizing': 0.14285714285714285, 'documents': 0.14285714285714285, 'messages': 0.14285714285714285, 'spam': 0.2857142857142857, 'vs.': 0.2857142857142857, 'non': 0.14285714285714285, 'positive': 0.14285714285714285, 'negative': 0.14285714285714285, 'reviews': 0.14285714285714285, 'advent': 0.14285714285714285, 'deep': 0.14285714285714285, 'dramatically': 0.14285714285714285, 'Particularly': 0.14285714285714285, 'transformer': 0.14285714285714285, 'based': 0.14285714285714285, 'architectures': 0.14285714285714285, 'BERT': 0.14285714285714285, 'Bidirectional': 0.14285714285714285, 'Encoder': 0.14285714285714285, 'Representations': 0.14285714285714285, 'Transformers': 0.14285714285714285, 'GPT': 0.14285714285714285, 'Generative': 0.14285714285714285, 'Pre': 0.14285714285714285, 'trained': 0.14285714285714285, 'Transformer': 0.14285714285714285, 'RoBERTa': 0.14285714285714285, 'models': 0.14285714285714285, 'capable': 0.14285714285714285, 'understanding': 0.14285714285714285, 'context': 0.14285714285714285, 'ambiguity': 0.14285714285714285, 'generating': 0.14285714285714285, 'impressive': 0.14285714285714285, 'fluency': 0.14285714285714285, 'widely': 0.14285714285714285, 'consumer': 0.14285714285714285, 'enterprise': 0.14285714285714285, 'applications': 0.14285714285714285, 'Popular': 0.14285714285714285, 'use': 0.14285714285714285, 'cases': 0.14285714285714285, 'chatbots': 0.14285714285714285, 'virtual': 0.14285714285714285, 'assistants': 0.14285714285714285, 'Siri': 0.14285714285714285, 'Alexa': 0.14285714285714285, 'Google': 0.14285714285714285, 'Assistant': 0.14285714285714285, 'rely': 0.14285714285714285, 'respond': 0.14285714285714285, 'voice': 0.14285714285714285, 'commands': 0.14285714285714285}\n",
            "-----------------\n",
            "SENTENCE SCORE :  {\n",
            "Natural Language Processing is a subfield of artificial intelligence (AI) that focuses on enabling machines to understand, interpret, and generate human language.: 2.999999999999999, NLP combines computational linguistics, computer science, and machine learning to process and analyze vast amounts of natural language data.: 2.428571428571428, As communication through text and speech is fundamental to human interaction, NLP plays a crucial role in bridging the gap between human and machine communication.: 3.999999999999999, One of the fundamental tasks in NLP is tokenization, where text is broken down into smaller units such as words or sentences.: 2.1428571428571423, This is often the first step in more complex processes like part-of-speech tagging, which assigns grammatical categories (like noun or verb) to each word.: 3.5714285714285707, Other essential techniques include stemming and lemmatization, which reduce words to their root or dictionary form to help normalize variations.: 2.285714285714285, More advanced NLP tasks include named entity recognition (NER), which identifies proper nouns like names of people, organizations, and locations, and sentiment analysis, which determines the emotional tone behind a body of text.: 4.7142857142857135, Text classification, another key task, allows for organizing documents or messages into categories such as spam vs. non-spam, or positive vs. negative reviews.: 3.5714285714285703, With the advent of deep learning, NLP has advanced dramatically.: 1.0, Particularly transformer-based architectures like BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), and RoBERTa.: 1.5714285714285712, These models are capable of understanding context, ambiguity, and even generating human-like text with impressive fluency.: 3.1428571428571423, NLP is widely used in both consumer and enterprise applications.: 0.5714285714285714, Popular use cases include chatbots and virtual assistants like Siri, Alexa, and Google Assistant, which rely on NLP to interpret and respond to voice commands.\n",
            ": 3.142857142857142}\n",
            "Length of Sentence Score :  13\n",
            "[More advanced NLP tasks include named entity recognition (NER), which identifies proper nouns like names of people, organizations, and locations, and sentiment analysis, which determines the emotional tone behind a body of text., As communication through text and speech is fundamental to human interaction, NLP plays a crucial role in bridging the gap between human and machine communication., This is often the first step in more complex processes like part-of-speech tagging, which assigns grammatical categories (like noun or verb) to each word.]\n",
            "-----------------\n",
            "TEXT SUMMARIZATION : \n",
            " More advanced NLP tasks include named entity recognition (NER), which identifies proper nouns like names of people, organizations, and locations, and sentiment analysis, which determines the emotional tone behind a body of text.As communication through text and speech is fundamental to human interaction, NLP plays a crucial role in bridging the gap between human and machine communication.This is often the first step in more complex processes like part-of-speech tagging, which assigns grammatical categories (like noun or verb) to each word.\n"
          ]
        }
      ]
    }
  ]
}